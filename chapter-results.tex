%--------------------------------------
\chapter{Results and Findings}
\label{chap_results}
%--------------------------------------

%--------------------------------------
\section{Analyzing Flaws}
\label{ch5:sec:flaws}
%--------------------------------------

\begin{sloppypar}
%\input{tables/tbl_flaws.tex}
\end{sloppypar}


The new analysis of the 13 crypto-detectors results in six new flaws being found. Additionally, the analysis found 19 flaws across the 5 new crypto-detectors that were located in the original set of flaws found by MASC. For this work I included an updated version of the flaw classes with the newly located flaws and present an updated table that maps the flaws to the new versions of the crypto-detectors and new crypto-detectors. Similarly, to the original work I found that the majority of total flaws found could be attributed to mutations rather than just using the base case versions of the misuse. This continues to prove that mutation testing provides better results for finding flaws than just using the base cases alone to determine flaws.

Since I reevaluated crypto-detectors found in the original paper I used the patched versions that included a fix for the multidex issue presented in the initial paper. However I noticed, notably with Amazon Code Guru, there does appear to be a limit to the number of recommendations it creates for a specific misuse. For example, when I used the exhaustive scope to evaluate the tool the misuse produced a "weak cipher algorithm" warning but it only reported 100 of these despite many more being seeded within each program. This would then mean after the user fixes the flaws they would have to know to run another analysis on their code. Similar to the first group this does affect the reliability of the results and makes it challenging to determine how well it evaluates many misuses. I will further discuss this fault with Amazon Code Guru and the crypto-detectors that did not fully analyze the code in the Discussion section. 

FC1: String Case Mishandling (F1): This class was the motivating example found for the original work and for consistency remains in its own class for this work. This example can be found in Table 1 as F1. For this case a developer may use des or dEs (instead of the expected DES) in Cipher.getInstance(<parameter>) without any errors being raised. This was not found by Snyk despite being able to detect the base case version of this misuse. Condiga and QARK were also unable to detect this flaw but were unable to detect this base case as well.

FC2: Incorrect Value Resolution (F2 – F9): For these flaws 11/13 crypto-detectors failed to successfully detect all 9 flaws. Notably though the crypto-detectors that were evaluated in the initial paper did perform better on average than the 5 new tools that were evaluated. Snyk and SonarQube performed on par with some of the tools that were initially evaluated. 

SonarQube was able to detect some of the more simple cases but failed due to the tool not evaluating method invocations. SonarQube is only capable of detecting String literals and can detect it if it is contained in a variable. 

FC3: Incorrect Resolution of Complex Inheritance and Anonymous Objects (F10 – F13): Flaws in this class occur because 10 crypto-detectors were unable to resolve the complex inheritance relationships among classes. These flaws were found specifically by using the flexible mutation operators. As found in the original paper this is clearly a consideration for some crypto-detectors while others did not consider this in design. 

FC4: Insufficient Analysis of Generic Conditions in Extensible Crypto-APIs (F14 – F16): The flaws in this section represent the inability of crypto-detectors to identify fake conditions and also true conditionals. This determines if a crypto-detector is capable of following path sensitivity.

FC5: Insufficient Analysis of Context-specific Conditions in Extensible Crypto-APIs (F17 – F19): The flaws found within this class are similar to those found in FC4, however, the fake conditionals are contextualized to the overidden function. This would simulate the behavior of an evasive developer because one my try to add further realism to fake conditions to avoid tools that are capable of detecting simple generic conditions.

FC6: Newly Introduce Flaws (F20 - F25): These flaws mostly focus on interprocedural behavior and are similar to flaws found within FC2, however, they were separated to help provide more of an emphasis on these newly introduced flaws. F23 - F25 are more complicated cases of F7 and due to F7 being present in 11/13 crypto-detectors it was not surprising that none of these three flaws were found by the crypto-detectors.

F20 was the least present flaw among the newly introduced flaws where the mutations were found by 4 of the crypto-detectors. F21 and F22 were found by only two crypto-detectors and interestingly were not able to be located by the two with the minimal case evaluation. They were only revealed when placed within Android apps in the similarity scope for the two crypto-detectors.

Finally, with the newly introduced operators that allowed for a user specified number of method calls and nested conditionals (F7 and F25), I had planned to test a varying number of method calls and nested conditionals. I did run minimal cases with 3, 4, 5, 10, 15, and 20 method calls/conditionals. However, since the simple versions of these were not able to be located I could not fully test the limits of how many calls or conditionals the crypto-detectors actually evaluated before stopping as planned when creating these operators. As these tools improve these operators will be able to be more fully utilized and help test the limits of the crypto-detectors.

\input{tables/tbl_flaws.tex}
\input{tables/new_tbl_flaws.tex}

\input{tables/new_tbl_flaw_tools.tex}

%--------------------------------------
\section{Comparing Original Results to New}
\label{ch5:sec:comparison}
%--------------------------------------

As a part of the extension the goal was also to look at the differences between what was found in the evaluation of the original work and the extension. For the most part new versions of tools did appear to see some improvements from their original evaluation. In fact seven out of eight tools had some improvement from the original evaluation, the exception being QARK because it is no longer being maintained. LGTM had the most differences between the original work and the new evaluation by eliminating four flaws that were originally found within the tools. The most from the other tools was eliminating one flaw or improving a partially present flaw to be completely eliminated. 

In addition, I also found that on average tools that were reevaluated by MASC had fewer flaws present than those who were being evaluated for the first time for this work. This could have occurred for a variety of reasons but one of the likely causes for this is the work done by the original MASC work. This is due to the fact that the flaws found in the original work was reported to the maintainers. While they did not eliminate every flaw that was reported they did overall see some improvements and did have fewer flaws than their counterparts that were being evaluated for the first time.

I also found that there were two cases where flaws that were not present in the original evaluation appeared in the reevaluation of the new version. This occurred in the LGTM evaluation with F3 and CogniCrypt with F7.  This shows that flaws do have a tendency to reappear and demonstrates that evaluation of crypto-detectors is not a one time thing, it needs to be ongoing. While it is clear that it is not a common occurrence for flaws to reappear it is clear from the results that it can happen. Each release has the potential to reveal more bugs and possibly bring back old misuses. In addition, across the reported flaws a total of 58 are still present in the new versions of the crypto-detectors even after these issues were reported. However, notably seven of the eight reevaluated crypto-detectors did see improvements since the original evaluation was done. Due to the issues that were reported by the original authors this shows that MASC at least help create some improvements in the tools. QARK is the only crypto-detector that saw no change since the original work.

%--------------------------------------
\section{Exhaustive Results}
\label{ch5:sec:exhaustive}
%--------------------------------------

\input{tables/tbl_mutations_overall.tex}

For the 5 new crypto-detectors it is notable how many misuses SonarQube is able to identify. Out of the apps that were able to easily be run on it, it had the highest percentage of found misuses. This seems to demonstrate that SonarQube is the most thorough when analyzing files. However, running Android apps on SonarQube led to many failures as well and this is why currently the number is so low.

It is also notable how well Snyk performed in this section. It misses many of the minimal examples but is capable of looking through many of the files and conducts a fairly thorough analysis. While it did miss 7,000 cases it still performed very well compared to the rest of the group. Snyk has more issues with the variety of misuses it can detect but does check every Java file for them.

I wanted to also take not of how poorly Code Guru appeared to perform. While this is the number of reported misuses it is possible that Code Guru detected more while not reporting them. Several of the applications run reached 100 insecure cipher algorithm warnings and stopped there. It is unlikely that Code Guru happened to find exactly 100 instances in each of these apps. This is why I believe once it reaches a certain number of a specific misuse it does not continue reporting. This makes it difficult to determine how well Code Guru looks through files.

Finally, I also want to address the differing number of total mutations tested on each crypto-detectors. This occurs due to a variety of reasons across the tools. Many of the tools have different criteria for how code is analyzed whether they look at source code, JARs, or APKs. Based on how these tools are built and all the apps being open source this leads to some issues with evaluating all the apps. For example, some applications from the older works have out of date versions for dependencies which leads to issues with evaluation. Specifically, I had issues with outdated Gradle versions working correctly with SonnarScanner. While it is likely possible to resolve these issues and run the code given time. Focus was not placed on fixing issues within each individual app if it was not a simple fix. This was because I did not develop the apps and there could be a variety of components preventing evaluation that I am not fully aware of and this would require being entirely familiar with each project. I ensured before evaluation that each app was compilable before and after mutation but that was the extent of changes made to apps. The reasons many apps were used was to ensure that I had a reasonable sample size to evaluate each crypto-detector.