%--------------------------------------
\chapter{Results and Findings}
\label{chap_results}
%--------------------------------------

%--------------------------------------
\section{Analyzing Flaws}
\label{ch5:sec:flaws}
%--------------------------------------

\begin{sloppypar}
\input{tables/tbl_flaws.tex}
\input{tables/new_tbl_flaw_tools.tex}
\end{sloppypar}


The new analysis of the 13 crypto dectors results in [work is currently ongoing] new flaws being found. Additionaly the analysis found [work ongoing] in the 5 new crypto-dectors that were located in the orignal flaws found by MASC. For this work I included an updated version of the flaw classes with the newly located flaws and present an updated table that maps the flaws to the new versions of the crypto-detectors and new crypto-detectors. Similarly, to the original work I found that the majority of total flaws found could be attributed to mutations [work ongoing] while [work ongoing] could be found by just using the base case versions of the misuse. This continues to prove that mutation testing provides better results for finding flaws than just using the base cases alone to determine flaws.

I also found that [work ongoing] of flaws found in the orignal paper are still present in the reevaluated new versions of the crypto-detectors. This shows that flaws do have a tedency to reappear and demonstrates that evaluation of crypto-detectors is not a one time thing, it needs to be ongoing. Each release has the potential to reveal more bugs and possibly bring back old misuses. In addition across the reported flaws a total of [work ongoing] are still present in the new versions of the crypto-dectors even after these issues were reported.

Since I reevaluated crypto-detectors found in the original paper I used the patched versions that included a fix for the multidex issue presented in the initial paper. However I noticed, notably with Amazon Code Guru, there does appear to be a limit to the number of recommendations it creates for a specific misuse. For example, when used the exhaustive scope the misuse produced a "weak cipher algorithm" warning but it only reported 100 of these despite many more being seeded within each program. This would then mean after the user fixes the flaws they would have to know to run another analysis on their code. Similar to the first group this does affect the reliability of the results and makes it challenging to determine how well it evaluates many misuses. I will further discuss this fault with Amazon Code Guru and the crypto-dectors that did not fully analyze the code in the Discission section. 

NOTE: Work is still ongoing for this portion. I am still waiting on a response from some of the tool maintainers. In addition some evaluation is still be conducted and results are still being processed. This will be updated prior to the final submission. Since work is still ongoing the new flaws are still being indentified and are not yet included in the flaw class table. Currently the flaw class table is just the one from the original work as a place holder. All tables are still a work in progress and do not represent the final results but are based on current values I have.

FC1: String Case Mishandling (F1): This class was the motivatiing example found for the orignal work and for consistency remains in its own class for this work. This example can be found in Table 1 as F1. For this case a developer may use des or dEs (instead of the expected DES) in Cipher.getInstacne(<parameter>) without any errors being raised. This was not found by Snyk despite being able to detect the base case version of this. Condiga and QARK were also unable to detect this flaw but were unable to detect this base case as well.

FC2: Incorrect Value Resolution (F2 – F9): For these flaws 11/13 crypto-dectors failed to sucessfully detect all 9 flaws. Notably though the crypto-dectors that were evaluated in the initial paper did perform better on average than the 5 new tools that were evaluated. Snyk and SonarQube performed on par with some of the tools that were initially evaluated. The crypto-dectors that had flaws reappear in this class are ... 

SonarQube was able to detect some of the more simple cases but failed due to the tool not evaluating method invocations. SonarQube is only capable of detecting String literals and can detect it if it is contained in a variable. 

For the rest of this section we are still waiting to hear back from tool maintainers to add the reasoning as to why their flaws were not evaluated by the tool.

FC3: Incorrect Resolution of Complex Inheritance and Anonymous Objects (F10 – F13): Flaws in this class occur because [insert value of crypto-dectors] were unable to resolve the complex inheritance relationships among classes. These flaws were found specifically by using the flexible mutation operators. As found in the original paper this is clearly a consideration for some of the crypto-dectors while others did not consider this in design. 

FC4: Insufficient Analysis of Generic Conditions in Extensible Crypto-APIs (F14 – F16): The flaws in this section represent the inability of crypto-detectors to intentify fake conditions and also true conditionals. This determines if a crypto-detector is capable of following path sensitivity.

FC5: Insufficient Analysis of Context-specific Conditionsin Extensible Crypto-APIs (F17 – F19): The flaws found within this class are simlar to those found in FC4, however, the fake conditionals are contextualized to the overidden function. This would simulate the behavior of an evasive developer because one my try to add further realism to fake conidtions to avoid tools that are cabable of detecting simple generic conditions.


%--------------------------------------
\section{Exhaustive Results}
\label{ch5:sec:discussion}
%--------------------------------------

\input{tables/tbl_mutations_overall.tex}

So far results have been found for the the 5 new operators. However, results for this section are still ongoing. The apps are currently being run on the original crypto-detectors. Results will be reported by the final version.

For the 5 new crypto-detectors it is notable how many misuses SonarQube is able to indentify. Out of the apps that were able to easily be run on it, it had the highest percentage of found misuses. This seems to demonstrate that SonarQube is the most thorough when analyzing files. However, running Anroid apps on SonarQube led to many failures as well and this is why currently the number is so low. It is currently being looked into if it will be possible to run the remaining applications. 

It is also notable how well Snyk performed in this section. It misses many of the minaml examples but is capable of looking through many of the files and conducts a fairly thorough analysis. While it did miss 7,000 cases it still performed very well compared to the rest of the group. Snyk has more issues with the variety of misuses it can detect but does check every Java file for them.

It is also notable how Code Guru performed. While this is the number of reported misuses it is possible that Code Guru detected more while not reporting them. Several of the applications run reached 100 insecure cipher algorithm warnings and stopped there. It is unlikely that Code Guru happened to find exactly 100 instances in each of these apps. This is why I believe once it reaches a certain number of a specific misuse it does not continue reporting. This makes it difficult to determine how well Code Guru looks through files.