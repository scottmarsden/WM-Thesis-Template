%--------------------------------------
\chapter{Results and Findings}
\label{chap_results}
%--------------------------------------

%--------------------------------------
\subsection{Analyzing Flaws}
\label{ch5:sec:flaws}
%--------------------------------------
%\input{tables/tbl_flaws}
%\input{tables/tbl_flaw_tools}


The new analysis of the 13 crypto dectors results in [work is currently ongoing] new flaws being found. Additionaly the analysis found [insert number of flaws] in the 5 new crypto-dectors that were located in the orignal flaws found by MASC. For this work I included an updated version of the flaw classes with the newly located flaws and present an updated table that maps the flaws to the new versions of the crypto-detectors and new crypto-detectors. Similarly, to the original work I found that the majority of total flaws found could be attributed to mutations [insert values] while [insert value] could be found by just using the base case versions of the misuse. This continues to prove that mutation testing provides better results for finding flaws than just using the base cases alone to determine flaws.
I also found that [insert number] of flaws found in the orignal paper are still present in the reevaluated new versions of the crypto-dectors. This shows that flaws do have a tedency to reappear and demonstrates that evaluation of crypto-dectors is not a one time thing, it needs to be ongoing. Each release has the potential to reveal more bugs and possibly bring back old misuses. In addition across the reported flaws a total of [insert number] are still present in the new versions of the crypto-dectors even after these issues were reported.
Since I reevaluated crypto-dectors found in the original paper I used the patched versions that included a fix for the multidex issue presented in the initial paper. However I noticed, notably with Amazon Code Guru, there does appear to be a limit to the number of recommendations it creates for a specific misuse. For example, when used the exhaustive scope the misuse produced a "weak cipher algorithm" warning but it only reported 100 of these despite many more being seeded within each program. This would then mean after the user fixes the flaws they would have to know to run another analysis on their code. Similar to the first group this does affect the reliability of the results and makes it challenging to determine how well it evaluates many misuses. I will further discuss this fault with Amazon Code Guru and the crypto-dectors that did not fully analyze the code in the Discission section [add section id]. These types of flaws are included in Flaw Class Zero since they cannot directly be attributed to being found by MASC.

NOTE: Work is still ongoing for this portion. I am still waiting on a response from some of the tool maintainers. In addition some evaluation is still be conducted and results are still being processed. This will be updated prior to the final submission. Since work is still ongoing the new flaws are still being intentified and are not yet included in the flaw class table.

FC1: String Case Mishandling (F1): This class was the motivatiing example found for the orignal work and for consistency remains in its own class for this work. This example can be found in Table 1 as F1. For this case a developer may use des or dEs (instead of the expected DES) in Cipher.getInstacne(<parameter>) without any errors being raised. This was not found by Snyk despite being able to detect the base case version of this. Condiga and QARK were also unable to detect this flaw but were unable to detect this base case as well.
FC2: Incorrect Value Resolution (F2 – F9): For these flaws 11/13 crypto-dectors failed to sucessfully detect all 9 flaws. Notably though the crypto-dectors that were evaluated in the initial paper did perform better on average than the 5 new tools that were evaluated. Snyk and SonarQube performed on par with some of the tools that were initially evaluated. The crypto-dectors that had flaws reappear in this class are ... 
SonarQube was able to detect some of the more simpler cases but failed due to the tool not evaluating method invocations. SonarQube is only capable of detecting String literals and can detect it if it is contained in a variable. 
For the rest of this section we are still waiting to hear back from tool maintainers to add the reasoning as to why their flaws were not evaluated by the tool.
FC3: Incorrect Resolution of Complex Inheritance and Anonymous Objects (F10 – F13): Flaws in this class occur because [insert value of crypto-dectors] were unable to resolbe the complex inheritance relationships among classes. These flaws were found specifically by using the flexible mutation operators. As found in the original paper this is clearly a consideration for some of the crypto-dectors while others did not consider this in design. 

FC4: Insufficient Analysis of Generic Conditions in Extensible Crypto-APIs (F14 – F16): The flaws in this section represent the inability of crypto-detectors to intentify fake conditions and also true conditionals. This determines if a crypto-detector is capable of following path sensitivity.

FC5: Insufficient Analysis of Context-specific Conditionsin Extensible Crypto-APIs (F17 – F19): The flaws found within this class are simlar to those found in FC4, however, the fake conditionals are contextualized to the overidden function. This would simulate the behavior of an evasive developer because one my try to add further realism to fake conidtions to avoid tools that are cabable of detecting simple generic conditions.


%--------------------------------------
\subsection{Exhaustive Results}
\label{ch5:sec:discussion}
%--------------------------------------

So far results have been found for the the 5 new operators. However, results for this section are still ongoing. The apps are currently being run on the original crypto-dectors. Results will be reported by the final version.