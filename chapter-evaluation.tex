%--------------------------------------
\chapter{Evaluation and Methodology}
\label{chap_evaluation}
%--------------------------------------

The goal of the evaluation was to find flaws in crypto-dectors using MASC once again. The main objectives were to (1) measure the effectiveness of the new operators at uncovering flaws in crypto-dectors and the old ones at uncovering flaws in new crypto-dectors (since the orignal operators were proven to be effective), (2) continue to learn the characteristics of the flaws found and their real world impacts, and (3) determine how likely flaws are to reappear in future versions of crypto-detectors. Based on these goals to evaluate MASC the same three original research questions were used to conduct evaluation with the addition of another:

RQ1: Can MASC discover new flaws in crypto-dectors?

RQ2: What are the characteristics of these flaws?

RQ3: What is the impact of the flaws on the effectiveness of crypto-detectors in pracitce?

RQ4: How likely is it for flaws to reappear in newer versions of crypto-detectors?

To answer RQ1-RQ4, I used the same methodology described in the intial MASC paper. I took the original set of 9 crypto-dectors and found their most up-to-date versions if possible (Xanitizer is no longer available) and then added an additional 5 crypto-dectors to the set that were not evaluated in the original paper. The crypto-dectors that were evaluated were CogniCrypt, SpotBugs with FindSecBugs, QARK, LGTM, GitHub Code Security, LGTM, ShiftLeft Scan, Amazon Code Guru, SonarQube, Codiga, Deepsource, and Snyk. All tools used are under active maintenance since the goal of MASC is to provide feedback on potential improvements to the cryto-dectors it is important that they are still in active use. Similar to the original paper the results of the evaluation are not intended to demonstrate comparative advantages of tools and are not intended as an endorcement of any such tools, each tool is evaluated seperately from the others using the same techniques.

For evaluation of the crypto-dectors the steps used to conduct evaluation remained the same. I expanded on the techniques used but ensured they were still consistent with the original work since RQ1-RQ3 are the same as the original paper.

Step 1 - Selecting and mutating apps: In the original work 13 open source Android apps and four sub-systems of Apache Qpid Broker-J were found and used for mutation. For the extension these same apps with their original mutations were used as a part of the evaluation. In addition, I located 15 Android apps on Github and Tink, a Java project from Google, for mutation. To locate these additional candidates for mutation I used a similar methodology as the original work. The way the new Android apps were located was by using GitHub's advanced search to locate Android apps that had a least 200 stars, these were then sorted by how recently they were updated to help ensure they would be both compilable and up to date with dependencies. All apps that were used were tested to ensure they were compilable prior to mutation. In addition to help get a variety of different types of apps I also searched for apps that contained tags such as: security, cryptography, secure, games, tools, calendar, excercise, and location. Out of the 15 new Android apps 5 of them were specifically found to include crypto-graphic API's (Cipher, MessageDigest, X509TrustManager) and were mutated using the Similarity scope. In addition to the 20,303 mutations found in the original MASC evaluation, I introduced 26,765 mutants within the new Android apps and Java project. This gives a grand total of 47,068 mutants generated by MASC used for evaluation, more than double the original amount of mutants. Generating all the new mutants took about 20 minutes total on MASC which addresses RQ3, and did not require any human intervention. MASC is capable of generating mutants very quickly the bottleneck of time comes from crypto-detectors evaluating the mutants.

Step 2 - Evaluating crypto-dectors and indentifying unkilled/undected mutants: To conduct the evaluation on a crypto-dector I analyzed the mutants that were produced using the crypto-detector. After this mutants that were not detected by the crypto-detector were indentified and flagged as unkilled. To conduct this full evaluation I kept track of logs produced by MASC for all the apps, this log contained information such as line numbers where mutations were placed and what type of mutations were placed in those locations. Then intially I ran the unmutated version of the app against the crypto-dector. Using the output produced I made a note of any crypto-graphic related misuses that were found on the unmutated version to ensure that these were not counted as a mutation produced by MASC. Then the mutated version of the app could be analyzed by the crypto-detector and results could be compared. Once the similarities of the reports were eliminated I looked at the remaining results and any remaining cryptographic misuse that was found is considered a mutant inserted by MASC that was killed by the crypto-detector. Each killed mutant is confirmed by comparing it with the log to ensure that a mutant was inserted at this location and the correct flag was produced by the crypto-detector. In addition, this analysis was also conducted automatically by the automated analysis mentioned in section 3.0.3 for any tool that could produced SARIF output such as CogniCrypt. The approach taken for this is done to ensure that misuses found by the crypto-detector that were not inserted by MASC are not considered as part of the evaluation. The main goal is to ensure all mutants that were inserted by MASC are located and marked as killed or unkilled. So far across the 5 new crypto-detectors the average number of found mutants is 14,070.

Step 3 - Indentifying Flaws (RQ1): For the apps using the exhaustive approach I randomly analyzed many of mutants that went undetected to locate porential flaws. I took the same approach as the original work and looked at misuses that the crypto-detector claims it can detect but in reality fails to detect it. To make sure all flaws were novel I exempted the exceptions that were stated specifically in the crypto-detector's documentation to ensure a fair evalaluation. After all the goal is to determine flaws where crypto-dectors make claims but fail to meet those claims. Since this work was done in the orignal MASC the claim remains consistent that "while a crypto-detector may seem flawed because it does not detect a newer, more recently identified misuse pattern, we confirm that all the flaws we report are due to misuse cases that are older than the tools in which we find them." This is because misuses that were used to evaluate the dectors are still the most discussed misuses between 1998-2022 and since all the tools used either have new versions or were recently deployed (Amazon Code Guru). In addition to confirm flaws I used the minimal app created for the origianl work that contained only the undetected misuses to re-analyze the all the tools. This app was also altered to include mutations created by the new operators to verify these as well.

Step 4 - Characterizing flaws (RQ2): Flaws were grouped into the same flaw classes, based on most likely cause, used for the original evaluation. The only difference was the flaw classes were updated to include the new mutants but the original flaw classes still remain intact. This was done to help further grasp why are the tools failing. In addition all 13 crypto-dectors were evaluated using the minimal examples represented by each flaw to gain a further understanding as to why a flaw might be missed despite the documentation make claims as such. Flaws were reported to their respective tool maintainers, and so far SonarQube has created 5 high priority issues in response to the reported flaws.

Step 5 - Understanding the practical impact of flaws (RQ3): To answer this research question for the extension I analyzed repositories to ensure that the new misuses that were introduced could also be found in real world applications. This was done using GitHub Code Search and was additionally confirmed manually.

Step 6 - Attibuting flaws to mutation vs base instantiation: To ensure that a flaw that MASC found was due to a mutation and not simply a lack of coverage of the base case, each crypto-detector was evaluated with the most basic version of the misuse to determine if it was able to catch that. An example of this would be directly testing something such as Cipher.getInstance("DES"), if this use case is caught it is then possible for it to detect mutations of this statement.

Step 7 - Reevaluation of the original 8 crypto-dectors: To reevaluate the original crypto-detectors I used the same approach that was conducted on them in the original work. I ensured that they had new versions since the original work was published. To ensure that what I found was a flaw that reappeared rather than a flaw that had not been fixed from the original work, I made sure that the issue that was reported by the original authors was closed. This was to ensure that the bug was addressed. I also report if any of the flaws present in the original work that were never fixed are still present in the new version.